# How Deployed Backend Saves Data

## ğŸ”„ Complete Data Flow: RSS â†’ Database

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     REQUEST FLOW                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. API REQUEST (HTTP POST)
   â””â”€â†’ POST /api/ingest
       â”‚
       â””â”€â†’ IngestRequest {
               sources: optional custom feeds,
               background: true/false
           }

2. ENDPOINT HANDLER (ingestion.py)
   â”‚
   â”œâ”€â†’ trigger_ingestion() endpoint
   â”‚   â”‚
   â”‚   â””â”€â†’ Create IngestionService(db)
   â”‚       â”‚
   â”‚       â”œâ”€â†’ Determine if background or foreground
   â”‚       â”‚
   â”‚       â””â”€â†’ If background: add_task to BackgroundTasks
   â”‚           â””â”€â†’ Returns immediately
   â”‚
   â”‚       If foreground: ingest_all(sources)
   â”‚           â””â”€â†’ Waits for completion
   â”‚           â””â”€â†’ Returns result directly

3. INGESTION SERVICE (ingestion_service.py)
   â”‚
   â”œâ”€â†’ ingest_all(sources)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Set status = RUNNING
   â”‚   â”œâ”€â†’ Record start_time
   â”‚   â”‚
   â”‚   â””â”€â†’ For each feed in sources (or DEFAULT_FEEDS):
   â”‚       â”‚
   â”‚       â””â”€â†’ _ingest_feed(feed_config)
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Fetch RSS with httpx.Client
   â”‚           â”‚   â””â”€â†’ GET https://feeds.feedburner.com/oreilly/radar
   â”‚           â”‚   â””â”€â†’ GET https://techcrunch.com/feed/
   â”‚           â”‚   â””â”€â†’ etc.
   â”‚           â”‚
   â”‚           â”œâ”€â†’ Parse with feedparser.parse()
   â”‚           â”‚
   â”‚           â”œâ”€â†’ For each entry in feed:
   â”‚           â”‚   â”‚
   â”‚           â”‚   â””â”€â†’ _process_entry(entry)
   â”‚           â”‚       â”‚
   â”‚           â”‚       â”œâ”€â†’ Extract: title, url, content, author, date
   â”‚           â”‚       â”‚
   â”‚           â”‚       â”œâ”€â†’ Check duplicate:
   â”‚           â”‚       â”‚   db.query(Article).filter(Article.url == url)
   â”‚           â”‚       â”‚   â””â”€â†’ If exists: skip (duplicates_skipped++)
   â”‚           â”‚       â”‚
   â”‚           â”‚       â”œâ”€â†’ If new article:
   â”‚           â”‚       â”‚   â”‚
   â”‚           â”‚       â”‚   â”œâ”€â†’ Get/create Category
   â”‚           â”‚       â”‚   â”‚   db.query(Category).filter(name == category)
   â”‚           â”‚       â”‚   â”‚   â””â”€â†’ If not exists: create new
   â”‚           â”‚       â”‚   â”‚
   â”‚           â”‚       â”‚   â”œâ”€â†’ Get/create Source
   â”‚           â”‚       â”‚   â”‚   db.query(Source).filter(name == source)
   â”‚           â”‚       â”‚   â”‚   â””â”€â†’ If not exists: create new
   â”‚           â”‚       â”‚   â”‚
   â”‚           â”‚       â”‚   â”œâ”€â†’ Create Article object:
   â”‚           â”‚       â”‚   â”‚   article = Article(
   â”‚           â”‚       â”‚   â”‚       title=title[:500],
   â”‚           â”‚       â”‚   â”‚       url=url[:1000],
   â”‚           â”‚       â”‚   â”‚       content=description,
   â”‚           â”‚       â”‚   â”‚       author=author,
   â”‚           â”‚       â”‚   â”‚       published_at=date,
   â”‚           â”‚       â”‚   â”‚       source_id=source.id,
   â”‚           â”‚       â”‚   â”‚       categories=[category]
   â”‚           â”‚       â”‚   â”‚   )
   â”‚           â”‚       â”‚   â”‚
   â”‚           â”‚       â”‚   â”œâ”€â†’ Add to session:
   â”‚           â”‚       â”‚   â”‚   db.add(article)
   â”‚           â”‚       â”‚   â”‚   â””â”€â†’ STAGED in memory (NOT saved yet)
   â”‚           â”‚       â”‚   â”‚
   â”‚           â”‚       â”‚   â””â”€â†’ Increment saved counter
   â”‚           â”‚
   â”‚           â””â”€â†’ Update source last_scraped timestamp
   â”‚
   â”œâ”€â†’ After all feeds processed:
   â”‚   â”‚
   â”‚   â”œâ”€â†’ COMMIT ALL CHANGES
   â”‚   â”‚   db.commit()  â† THIS IS KEY!
   â”‚   â”‚   â”‚
   â”‚   â”‚   â””â”€â†’ All staged articles â†’ persisted to database
   â”‚   â”‚       â””â”€â†’ SQLite file OR PostgreSQL
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Set status = COMPLETED or PARTIAL
   â”‚   â”œâ”€â†’ Record end_time
   â”‚   â”‚
   â”‚   â””â”€â†’ Return IngestionResult
   â”‚       â”œâ”€â†’ status: "completed"
   â”‚       â”œâ”€â†’ total_articles_saved: 30
   â”‚       â”œâ”€â†’ duplicates_skipped: 0
   â”‚       â”œâ”€â†’ errors: 0
   â”‚       â””â”€â†’ success_rate: 100%

4. DATABASE PERSISTENCE
   â”‚
   â”œâ”€â†’ SQLAlchemy generates SQL INSERT statements:
   â”‚   â”‚
   â”‚   â”œâ”€â†’ INSERT INTO sources (name, url, ...) VALUES (...)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ INSERT INTO categories (name, slug, ...) VALUES (...)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ INSERT INTO articles (title, url, content, ...) VALUES (...)
   â”‚   â”‚
   â”‚   â””â”€â†’ INSERT INTO article_categories (article_id, category_id) VALUES (...)
   â”‚
   â”œâ”€â†’ Execute against database:
   â”‚   â”‚
   â”‚   â”œâ”€â†’ For SQLite deployment:
   â”‚   â”‚   â””â”€â†’ Write to ./data/ai_news.db file
   â”‚   â”‚       â””â”€â†’ File saved on persistent volume
   â”‚   â”‚
   â”‚   â””â”€â†’ For PostgreSQL deployment:
   â”‚       â””â”€â†’ Write to PostgreSQL server
   â”‚           â””â”€â†’ Data persisted on database server
   â”‚
   â””â”€â†’ Return results to client

```

---

## ğŸ—„ï¸ Database Save Mechanism (SQLAlchemy Transaction)

### Step 1: **Staging** (In-Memory)
```python
# Inside _process_entry()
article = Article(
    title="New AI Breakthrough",
    url="https://example.com/article",
    content="Article text...",
    source_id=1,
    published_at=datetime.now()
)
db.add(article)  # â† Added to session, NOT saved yet
# Article is in SQLAlchemy session identity map
```

### Step 2: **Flushing** (Transaction Prepared)
```python
# Happens automatically when needed
db.flush()  # â† Generates SQL but doesn't commit
# INSERT INTO articles (...) VALUES (...)
```

### Step 3: **Committing** (Persisted)
```python
# After all entries processed from all feeds
db.commit()  # â† PERSISTS to database file/server
# SQLite: Written to ./data/ai_news.db
# PostgreSQL: Stored in database server
```

### Step 4: **Error Handling** (Rollback if Fails)
```python
try:
    # Process and add articles
    db.commit()
except Exception as e:
    db.rollback()  # â† Undo all changes
    raise
```

---

## ğŸ“Š Data Flow Diagram

```
RSS Feeds (5 sources)
    â†“
feedparser.parse() â† Parse XML/Atom
    â†“
30 entries extracted
    â†“
For each entry:
â”œâ”€â†’ Check duplicate (query database)
â”œâ”€â†’ Get/create category (query database)
â”œâ”€â†’ Get/create source (query database)
â”œâ”€â†’ Create Article object (in memory)
â””â”€â†’ db.add(article) â† Stage in session
    â†“
All articles staged in db.session
    â†“
db.commit() â† THE CRITICAL STEP
    â†“
SQLAlchemy generates & executes SQL
    â†“
INSERT statements executed
    â†“
âœ… Data written to:
   - SQLite: ./data/ai_news.db
   - PostgreSQL: database server
```

---

## ğŸš€ Deployment-Specific Save Behavior

### **Docker Compose Deployment**
```yaml
# docker-compose.yml
services:
  backend:
    volumes:
      - backend_data:/app/data  â† Persistent volume
    environment:
      - DATABASE_URL=sqlite:///app/data/news.db

# On container:
# 1. SQLite file: /app/data/news.db
# 2. Mounted to: backend_data volume
# 3. Persisted on host: Docker volume
# 4. Survives container restart: YES âœ…
```

**Data persists because:**
- âœ… Database file is in a Docker volume
- âœ… Volume persists even if container stops
- âœ… Multiple containers can share volume
- âœ… Backed up automatically (backup service)

### **Render.com Deployment (Free Tier)**
```yaml
# render.yaml
services:
  ai-tech-news-backend:
    env: python
    # Default: SQLite on ephemeral disk
    # DATABASE_URL=sqlite:///./data/news.db
```

**âš ï¸ Data does NOT persist because:**
- âŒ Ephemeral disk = temporary file storage
- âŒ Data lost when container restarts
- âŒ New deployment = new disk
- âŒ No volume mounting option

**Solution:**
```yaml
# Use PostgreSQL instead (paid tier required)
services:
  postgres:
    plan: basic_256mb  # $7/month
    
  backend:
    environment:
      DATABASE_URL=postgresql://user:pass@host/db
```

### **Render.com Deployment (Paid Tier with PostgreSQL)**
```yaml
# render.yaml with PostgreSQL
services:
  postgres:
    plan: basic_256mb
    region: oregon
    
  backend:
    environment:
      DATABASE_URL=postgresql://user:pass@render.onrender.com/ai_news
```

**Data persists because:**
- âœ… PostgreSQL managed by Render
- âœ… Data stored on persistent database server
- âœ… Survives all redeployments
- âœ… Automatic backups by Render

---

## ğŸ” Actual Save Code Execution

### When API Called: `POST /api/ingest`

```python
# File: src/api/routes/ingestion.py
@router.post("/")
async def trigger_ingestion(
    request: IngestRequest,
    background_tasks: BackgroundTasks,
    db=Depends(get_db)  # â† Get database session dependency
):
    ingestion_service = IngestionService(db)  # â† Pass database session
    
    # Either:
    # 1. Background: add_task(_run_ingestion, ...)
    # 2. Foreground: result = ingestion_service.ingest_all(request.sources)
    
    result = ingestion_service.ingest_all(request.sources)
    # â†‘ This returns IngestionResult with saved count
```

### Inside `ingest_all()`:

```python
# File: src/services/ingestion_service.py
def ingest_all(self, sources=None) -> IngestionResult:
    # ... process all feeds ...
    
    # THE CRITICAL LINE:
    self.db.commit()  # â† Persist all articles to database
    
    # After this line:
    # âœ… All 30 articles are saved
    # âœ… SQLite file updated (if using SQLite)
    # âœ… PostgreSQL rows inserted (if using PostgreSQL)
    # âœ… Cannot be undone (transaction committed)
    
    return self.result
```

---

## ğŸ’¾ Where Data Lives After Save

### Development (Local Machine)
```
File: ./backend/data/ai_news.db
â”œâ”€â†’ SQLite database file
â”œâ”€â†’ Size: ~1-5MB for typical articles
â”œâ”€â†’ Persists on disk
â””â”€â†’ Query with: sqlite3 ./data/ai_news.db
```

### Production (Docker)
```
Docker Volume: backend_data
â”œâ”€â†’ Location on host: /var/lib/docker/volumes/backend_data/
â”œâ”€â†’ Inside container: /app/data/news.db
â”œâ”€â†’ Persists across restarts
â””â”€â†’ Backed up daily to ./backups/
```

### Production (Render - Paid)
```
PostgreSQL Server (Render Managed)
â”œâ”€â†’ Host: ai-tech-news-db.onrender.com
â”œâ”€â†’ Database: ai_news
â”œâ”€â†’ User: postgres
â”œâ”€â†’ Password: (environment variable)
â”œâ”€â†’ Persists on Render servers
â””â”€â†’ Automatic backups every 24 hours
```

---

## âœ… Verification Steps

### After Ingestion Completes

**1. Check API Response:**
```bash
curl -X POST http://localhost:8000/api/ingest
# Response:
# {
#   "message": "Ingestion completed: 30 articles saved",
#   "background": false
# }
```

**2. Query Database Directly:**
```bash
# SQLite
sqlite3 ./data/ai_news.db "SELECT COUNT(*) FROM articles;"
# Output: 30

# PostgreSQL
psql postgresql://user:pass@host/db -c "SELECT COUNT(*) FROM articles;"
# Output: 30
```

**3. Check Database File Size:**
```bash
# SQLite file size increases
ls -lh ./data/ai_news.db
# -rw-r--r--  1 user  staff  2.5M Oct 22 10:15 ./data/ai_news.db
```

**4. Get Status:**
```bash
curl http://localhost:8000/api/ingest/status
# Response:
# {
#   "status": "completed",
#   "result": {
#     "status": "completed",
#     "total_articles_saved": 30,
#     "duplicates_skipped": 0,
#     "errors": 0,
#     "success_rate": 100.0
#   }
# }
```

---

## ğŸ”„ Transactional Safety

### What Happens If Save Fails

```python
try:
    # Process entries
    db.add(article1)
    db.add(article2)
    db.add(article3)
    
    # Attempt to commit
    db.commit()  # â† If THIS fails...
    
except Exception as e:
    # ...all changes are ROLLED BACK
    db.rollback()
    # article1, article2, article3 NOT saved
    # Database remains unchanged
    # State preserved: all-or-nothing
```

### Transaction Isolation

```
Request 1:
â”œâ”€â†’ Process feeds
â”œâ”€â†’ Add 30 articles to session
â””â”€â†’ db.commit()
    â†“
    âœ… 30 articles persisted
    
Request 2 (simultaneous):
â”œâ”€â†’ Reads database
â””â”€â†’ Sees Request 1's data ONLY after commit
```

---

## ğŸ“ˆ Performance Considerations

### Batch Processing vs. Individual Saves

**âŒ BAD** (Slow - 30 commits):
```python
for article in articles:
    db.add(article)
    db.commit()  # Commit after each article
    # 30 commits = 30 database writes
```

**âœ… GOOD** (Fast - 1 commit):
```python
for article in articles:
    db.add(article)
db.commit()  # One commit for all
# 1 commit = 1 batch write
# ~100x faster
```

**Current Implementation**: âœ… Uses batch commit (1 commit for all articles)

---

## ğŸ¯ Summary: How Deployed Backend Saves Data

1. **Request arrives** â†’ POST /api/ingest
2. **API endpoint** creates IngestionService with database session
3. **Service fetches** RSS feeds (5 sources, 30 articles)
4. **For each article:**
   - Check if duplicate (query DB)
   - Extract metadata (title, URL, content, author, date)
   - Create Article object (in-memory)
   - db.add(article) to session
5. **After all articles staged:**
   - db.commit() â† PERSISTS all to database
6. **Data persisted to:**
   - SQLite: ./data/ai_news.db (or Docker volume)
   - PostgreSQL: Render database server (if paid tier)
7. **Return result** with save statistics (30 articles saved, 100% success)

**Key Point**: All articles are added to session first, then committed in **ONE transaction** for efficiency and data safety.

---

**Last Updated**: October 22, 2025
**Status**: âœ… Production-ready, verified working
