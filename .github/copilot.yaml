# GitHub Copilot Configuration for AI Tech-News Assistant
# This file guides Copilot and reviewers on code conventions, architecture, and best practices

# Project Overview
# This is a job-market-aligned AI Tech-News Assistant that aggregates, analyzes, and presents technology news
# with AI-powered insights to help professionals stay current with industry trends.

# Stack Architecture & Layers
stack:
  # Backend Layer - Python for AI/ML processing
  backend:
    - language: Python
      purpose: AI/ML processing, news analysis, NLP operations
      frameworks: [FastAPI, Flask, Django]
      libraries: [transformers, pandas, numpy, scikit-learn, spaCy]
      patterns: ["Clean Architecture", "Repository Pattern", "Dependency Injection"]
    
  # API Layer - RESTful services and GraphQL
  api:
    - language: Python/Node.js
      purpose: REST APIs, GraphQL endpoints, data aggregation
      frameworks: [FastAPI, Express.js, Apollo Server]
      patterns: ["RESTful design", "OpenAPI/Swagger documentation", "Rate limiting"]
    
  # Frontend Layer - Modern web interface
  frontend:
    - language: JavaScript/TypeScript
      purpose: User interface, data visualization, interactive dashboards
      frameworks: [React, Vue.js, Next.js]
      libraries: [D3.js, Chart.js, Material-UI, Tailwind CSS]
      patterns: ["Component-based architecture", "State management", "Responsive design"]
    
  # Infrastructure Layer - Containerization and deployment
  infrastructure:
    - containerization: Docker
      purpose: Application packaging, microservices deployment
      orchestration: [Docker Compose, Kubernetes]
      patterns: ["Multi-stage builds", "Health checks", "Environment-based configuration"]

# Code Review Priorities (in order of importance)
review_priorities:
  1. security:
      - API key management and secure storage
      - Input validation and sanitization
      - Authentication and authorization
      - Data privacy compliance (GDPR, CCPA)
      
  2. performance:
      - AI model inference optimization
      - Database query efficiency
      - Caching strategies (Redis, CDN)
      - API response times
      
  3. maintainability:
      - Code readability and documentation
      - Test coverage (unit, integration, e2e)
      - Error handling and logging
      - Modular architecture
      
  4. scalability:
      - Horizontal scaling capabilities
      - Database design for growth
      - Message queue implementation
      - Load balancing considerations

# Best Practices & Conventions
best_practices:
  
  # Python Best Practices
  python:
    - "Use type hints for all function signatures"
    - "Follow PEP 8 style guidelines"
    - "Implement comprehensive error handling with custom exceptions"
    - "Use dataclasses or Pydantic models for data structures"
    - "Write docstrings following Google or NumPy style"
    - "Use virtual environments and requirements.txt/poetry"
    - "Implement proper logging with structured formats"
    
  # Node.js/JavaScript Best Practices
  javascript:
    - "Use ES6+ features and async/await for asynchronous operations"
    - "Implement proper error handling with try-catch blocks"
    - "Use TypeScript for type safety in larger codebases"
    - "Follow ESLint and Prettier configurations"
    - "Implement proper input validation with libraries like Joi or Yup"
    - "Use environment variables for configuration"
    
  # AI/ML Specific Practices
  ai_ml:
    - "Version control ML models and datasets"
    - "Implement model monitoring and performance tracking"
    - "Use proper train/validation/test splits"
    - "Document model assumptions and limitations"
    - "Implement bias detection and fairness testing"
    - "Use reproducible random seeds for consistency"
    
  # API Design Practices
  api_design:
    - "Follow RESTful conventions for resource naming"
    - "Implement proper HTTP status codes"
    - "Use pagination for large datasets"
    - "Implement request/response logging"
    - "Version APIs appropriately (v1, v2, etc.)"
    - "Provide comprehensive API documentation"
    
  # Docker & Infrastructure
  docker:
    - "Use multi-stage builds for smaller images"
    - "Implement health checks for containers"
    - "Use .dockerignore to exclude unnecessary files"
    - "Set proper user permissions (non-root)"
    - "Use environment-specific configurations"
    - "Tag images with semantic versioning"

# Testing Strategy
testing:
  unit_tests:
    - "Aim for 80%+ code coverage"
    - "Test business logic and edge cases"
    - "Mock external dependencies"
    - "Use pytest for Python, Jest for JavaScript"
    
  integration_tests:
    - "Test API endpoints with real databases"
    - "Validate data processing pipelines"
    - "Test AI model inference workflows"
    
  e2e_tests:
    - "Test critical user journeys"
    - "Validate UI interactions"
    - "Test deployment and rollback procedures"

# Documentation Requirements
documentation:
  - "README with setup and deployment instructions"
  - "API documentation with examples"
  - "Architecture decision records (ADRs)"
  - "Contributing guidelines"
  - "Security and deployment guides"
  - "Model documentation and performance metrics"

# Environment & Configuration
environment:
  development:
    - "Use local Docker Compose for development"
    - "Implement hot reloading for faster iteration"
    - "Use test databases and mock services"
    
  staging:
    - "Mirror production environment closely"
    - "Use production-like data volumes"
    - "Implement monitoring and alerting"
    
  production:
    - "Use managed services for databases and caching"
    - "Implement proper backup and disaster recovery"
    - "Monitor performance and security metrics"

# AI/ML Specific Guidelines
ml_operations:
  model_development:
    - "Use Jupyter notebooks for experimentation"
    - "Track experiments with MLflow or Weights & Biases"
    - "Version datasets and feature engineering pipelines"
    
  model_deployment:
    - "Containerize models for consistent deployment"
    - "Implement A/B testing for model updates"
    - "Monitor model drift and performance degradation"
    
  data_management:
    - "Implement data validation and quality checks"
    - "Use proper data versioning strategies"
    - "Ensure data privacy and anonymization"